{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, activate the GPU on Colab!\n",
    "- This notebook is supposed to be opened on Google Colab. \n",
    "- From the \"Runtime\" tab on top of the notebook, choose \"Change runtime type\" to select a GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPRT-TANDEM tutorial\n",
    "- Let's upload the repo onto Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # suppress warning messages\n",
    "\n",
    "!git clone https://github.com/Akinori-F-Ebihara/SPRT-TANDEM-tutorial\n",
    "os.chdir('./SPRT-TANDEM-tutorial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Nosaic-MNIST database\n",
    "- The script ``make_nmnist.py`` will generate train and test Nosaic-MNIST databases.\n",
    "- Two files, ``./data-directory/nosaic_mnist_train.tfrecords`` and ``./data-directory/nosaic_mnist_test.tfrecords`` will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python make_nmnist_testonly.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize example sequential data\n",
    "- Choose one random test data to visualize\n",
    "- The shape of one data is _(duration, width, height, channel) = (20, 28, 28, 1)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 20 # the length of sequential data\n",
    "batch_size = 100\n",
    "\n",
    "import tensorflow as tf\n",
    "from datasets.data_processing import read_tfrecords_nosaic_mnist,\\\n",
    "    decode_nosaic_mnist, binarize_labels_nosaic_mnist,\\\n",
    "    normalize_images_nosaic_mnist, reshape_for_featext\n",
    "\n",
    "# Reed the test tfr\n",
    "def _parse_image_function(example_proto):\n",
    "        return tf.io.parse_single_example(example_proto, {\n",
    "                    'video': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.io.FixedLenFeature([],tf.int64)\n",
    "                    })\n",
    "\n",
    "record_file_test='./data-directory/nosaic_mnist_test.tfrecords'\n",
    "\n",
    "raw_image_dataset_test = tf.data.TFRecordDataset(record_file_test)\n",
    "parsed_image_dataset_test = raw_image_dataset_test.map(_parse_image_function)\n",
    "parsed_image_dataset_test = parsed_image_dataset_test.batch(\n",
    "    batch_size, drop_remainder=True) ###\n",
    "    \n",
    "for iter_b, feats in enumerate(parsed_image_dataset_test):\n",
    "    x_batch, y_batch = decode_nosaic_mnist(feats, duration=duration)\n",
    "    if np.random.rand() < 0.5: break\n",
    "\n",
    "# select one data randomly from the batch\n",
    "dice = np.random.permutation(batch_size)[0]\n",
    "x_batch = x_batch[dice]\n",
    "x_batch = tf.expand_dims(x_batch, axis=0)\n",
    "y_batch = y_batch[dice]\n",
    "y_batch = tf.expand_dims(y_batch, axis=0)\n",
    "\n",
    "print('Shape of a data in NMNIST:', x_batch[0].shape)\n",
    "print('Digit label: ', y_batch.numpy())\n",
    "\n",
    "plt.figure(figsize = (24.3,15), facecolor='white')\n",
    "plt.rcParams['font.size'] = 25\n",
    "for i in range(duration):\n",
    "  plt.subplot(4, 5, i+1)\n",
    "  plt.imshow(np.squeeze(x_batch[0, i]), cmap='gray')\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.title(i+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pretrained model\n",
    "### Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.backbones_fe import ResNetModel, get_ressize_dependent_params, checkpoint_logger_tmp\n",
    "from models.backbones_ti import LSTMModelLite\n",
    "from models.losses import get_loss_fe, get_gradient_lstm_ver2\n",
    "\n",
    "root_ckptlogs = \"./data-directory\"\n",
    "path_resume_FE =  \"./data-directory/trained_models/FE_NMNIST/\"\n",
    "resnet_size = 44\n",
    "nb_cls = 2\n",
    "final_size = 32\n",
    "resnet_version = 1\n",
    "\n",
    "def load_params(model, ckpt_path):\n",
    "  ckpt = tf.train.Checkpoint(net=model)\n",
    "\n",
    "  assert os.path.exists(ckpt_path), \"Not exist: path_ckpt {}\".format(ckpt_path)\n",
    "\n",
    "  # Create ckpt and manager for restore\n",
    "  ckpt_manager_restore = tf.train.CheckpointManager(\n",
    "      ckpt, ckpt_path, max_to_keep=3)\n",
    "\n",
    "  # Restore the latest ckpt log.\n",
    "  ckpt.restore(ckpt_manager_restore.latest_checkpoint)\n",
    "  print(\"Restored from {}\".format(ckpt_manager_restore.latest_checkpoint))        \n",
    "\n",
    "\n",
    "# retrieve ResNet related parameters    \n",
    "dict_resparams = get_ressize_dependent_params(resnet_version, resnet_size)\n",
    "\n",
    "# feature extractor\n",
    "model_FE = ResNetModel(\n",
    "    resnet_size=resnet_size,\n",
    "    bottleneck=dict_resparams[\"bottleneck\"],\n",
    "    num_classes=nb_cls,\n",
    "    kernel_size=dict_resparams[\"kernel_size\"],\n",
    "    conv_stride=dict_resparams[\"conv_stride\"],\n",
    "    first_pool_size=dict_resparams[\"first_pool_size\"],\n",
    "    first_pool_stride=dict_resparams[\"first_pool_stride\"],\n",
    "    block_sizes=dict_resparams[\"block_sizes\"],\n",
    "    block_strides=dict_resparams[\"block_strides\"],\n",
    "    final_size=final_size,\n",
    "    resnet_version=resnet_version,\n",
    "    data_format='channels_last',\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# load the pretrained parameters\n",
    "load_params(model_FE, path_resume_FE)\n",
    "\n",
    "if nb_cls == 2:\n",
    "    y_batch = binarize_labels_nosaic_mnist(y_batch)\n",
    "\n",
    "x_batch_fe, y_batch_fe = reshape_for_featext(x_batch, y_batch, (28, 28, 1)) \n",
    "    # (bs*duration, 28,28,1), (bs*duration,)\n",
    "x_batch_fe = normalize_images_nosaic_mnist(x_batch_fe)\n",
    "            \n",
    "# 2. Extract features\n",
    "_, losses, _, feats_batch = get_loss_fe(\n",
    "model_FE, \n",
    "x_batch_fe, \n",
    "y_batch_fe, \n",
    "training=False, \n",
    "param_wd=None, \n",
    "flag_wd=False,\n",
    "calc_grad=False\n",
    ")\n",
    "\n",
    "ax = plt.figure(figsize=(16.2, 10), facecolor='white').gca()\n",
    "plt.imshow(np.transpose(feats_batch), aspect='auto')\n",
    "plt.ylabel('Feature')\n",
    "plt.xlabel('Frame')\n",
    "plt.title('Features extracted from video frames')\n",
    "ax.set_xticks(np.arange(0, duration))\n",
    "ax.set_xticklabels(np.arange(1, duration+1))\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "\n",
    "# Reshape (batch, duration, final size)\n",
    "feats_batch = tf.reshape(feats_batch, (1, duration, final_size)) # now batch_size=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_sprt = 1\n",
    "width_lstm = 64\n",
    "path_resume_TI = \"./data-directory/trained_models/TI_NMNIST/\"\n",
    "\n",
    "model_TI = LSTMModelLite(\n",
    "    nb_cls, \n",
    "    width_lstm, \n",
    "    dropout=0.0, \n",
    "    activation='tanh')\n",
    "\n",
    "# load the pretrained parameters\n",
    "load_params(model_TI, path_resume_TI)\n",
    "\n",
    "# Calc loss and grad, and backpropagation\n",
    "_, losses, logits_concat, LLRs = \\\n",
    "                get_gradient_lstm_ver2(\n",
    "    model_TI, \n",
    "    feats_batch, \n",
    "    y_batch, \n",
    "    training=True, \n",
    "    order_sprt=order_sprt,\n",
    "    duration=duration,\n",
    "    oblivious=False,\n",
    "    version='E',\n",
    "    param_multiplet_loss=1.0, \n",
    "    param_llr_loss=1.0, \n",
    "    param_wd=0.00001, \n",
    "    flag_wd=False,\n",
    "    flag_mgn=False,\n",
    "    calc_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate log-likelihood ratio (LLR) and plot the LLR trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshpos = 0.75\n",
    "\n",
    "impool = []\n",
    "tmp = np.squeeze(x_batch.numpy())\n",
    "for i in range(duration):\n",
    "  impool.append(tmp[i])\n",
    "\n",
    "plt.rcParams['font.size'] = 25\n",
    "fig = plt.figure(figsize=(24.3, 15), facecolor='white')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('Example LLR trajectory')\n",
    "\n",
    "gs = fig.add_gridspec(nrows=10, ncols=1)\n",
    "ax0 = fig.add_subplot(gs[0, :])\n",
    "ax1 = fig.add_subplot(gs[1:, :])\n",
    "\n",
    "ax0.set_xticks([])\n",
    "ax0.set_yticks([])\n",
    "ax0.imshow(np.concatenate(impool, -1), cmap='gray')\n",
    "\n",
    "ax1.set_ylabel('Even <<<     LLR     >>> Odd')\n",
    "ax1.set_xlabel('Frame')\n",
    "ax1.plot([0, duration], [0, 0], 'k')\n",
    "maxrange = np.max(np.abs(LLRs[:, :, 1, 0]))\n",
    "ax1.plot([0, duration], [maxrange*threshpos, maxrange*threshpos], 'k--')\n",
    "ax1.plot([0, duration], [-maxrange*threshpos, -maxrange*threshpos], 'k--')\n",
    "ax1.text(0.1, 0.5+maxrange*threshpos, 'Threshold')\n",
    "ax1.text(0.1, -0.5-maxrange*threshpos, 'Threshold', verticalalignment='top')\n",
    "ax1.plot(np.transpose(LLRs[:, :, 1, 0]), linewidth=5, color='g')\n",
    "ax1.set_xticks(np.arange(0, duration))\n",
    "ax1.set_xticklabels(np.arange(1, duration+1))\n",
    "ax1.set_xlim([-0.5, duration-0.5])\n",
    "ax1.set_ylim([-maxrange-2, maxrange+2])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the population average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLRpool = []\n",
    "labelpool = []\n",
    "\n",
    "for iter_b, feats in enumerate(tqdm.tqdm(parsed_image_dataset_test)):\n",
    "    x_batch, y_batch = decode_nosaic_mnist(feats, duration=duration)\n",
    "\n",
    "    if nb_cls == 2:\n",
    "        y_batch = binarize_labels_nosaic_mnist(y_batch)\n",
    "\n",
    "    x_batch_fe, y_batch_fe = reshape_for_featext(x_batch, y_batch, (28, 28, 1)) \n",
    "        # (bs*duration, 28,28,1), (bs*duration,)\n",
    "    x_batch_fe = normalize_images_nosaic_mnist(x_batch_fe)\n",
    "                \n",
    "    # 2. Extract features\n",
    "    _, losses, _, feats_batch = get_loss_fe(\n",
    "        model_FE, \n",
    "        x_batch_fe, \n",
    "        y_batch_fe, \n",
    "        training=False, \n",
    "        param_wd=None, \n",
    "        flag_wd=False,\n",
    "        calc_grad=False\n",
    "        )\n",
    "    feats_batch = tf.reshape(feats_batch, (batch_size, duration, final_size))\n",
    "\n",
    "    # Calc loss and grad, and backpropagation\n",
    "    _, losses, logits_concat, LLRs = \\\n",
    "                    get_gradient_lstm_ver2(\n",
    "        model_TI, \n",
    "        feats_batch, \n",
    "        y_batch, \n",
    "        training=True, \n",
    "        order_sprt=order_sprt,\n",
    "        duration=duration,\n",
    "        oblivious=False,\n",
    "        version='E',\n",
    "        param_multiplet_loss=1.0, \n",
    "        param_llr_loss=1.0, \n",
    "        param_wd=0.00001, \n",
    "        flag_wd=False,\n",
    "        flag_mgn=False,\n",
    "        calc_grad=False)\n",
    "        \n",
    "    LLRpool.append(LLRs)# LLRs[:, :, 1, 0]\n",
    "    labelpool.append(y_batch.numpy())\n",
    "\n",
    "LLRpool = np.concatenate(LLRpool, axis=0)\n",
    "labelpool = np.concatenate(labelpool, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the average LLR trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean0 = np.mean(LLRpool[labelpool==0, :, 1, 0], axis=0)\n",
    "std0 = np.std(LLRpool[labelpool==0, :, 1, 0], axis=0)\n",
    "mean1 = np.mean(LLRpool[labelpool==1, :, 1, 0], axis=0)\n",
    "std1 = np.std(LLRpool[labelpool==1, :, 1, 0], axis=0)\n",
    "\n",
    "fig = plt.figure(figsize=(24.3, 15), facecolor='white')\n",
    "maxrange = np.max([np.max(mean1), np.max(-mean0)])\n",
    "minrange = np.min([np.min(mean1), np.min(-mean0)])\n",
    "plt.plot([0, duration], [maxrange*threshpos, maxrange*threshpos], 'k--')\n",
    "plt.plot([0, duration], [-maxrange*threshpos, -maxrange*threshpos], 'k--')\n",
    "plt.text(0.1, 0.5+maxrange*threshpos, 'Threshold')\n",
    "plt.text(0.1, -0.5-maxrange*threshpos, 'Threshold', verticalalignment='top')\n",
    "plt.plot(mean1, color='darkred', linewidth=5, label='Odd')\n",
    "plt.fill_between(np.arange(0, duration), mean1-std1, mean1+std1, color='darkred', alpha=0.5)\n",
    "plt.plot(mean0, color='midnightblue', linewidth=5, label='Even')\n",
    "plt.fill_between(np.arange(0, duration), mean0-std0, mean0+std0, color='midnightblue', alpha=0.5)\n",
    "plt.title('Average LLR trajectory (shade: standard deviation)')\n",
    "plt.ylabel('Even <<<     LLR     >>> Odd')\n",
    "plt.xlabel('Frame')\n",
    "plt.legend()\n",
    "plt.gca().set_xticks(np.arange(0, duration))\n",
    "plt.gca().set_xticklabels(np.arange(1, duration+1))\n",
    "plt.xlim([0, duration-1])\n",
    "plt.ylim([-maxrange-3, maxrange+3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the speed-accuracy tradeoff (SAT) curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import LogFormatter, ScalarFormatter, NullFormatter\n",
    "from utils.performance_metrics import multiplet_sequential_confmx,\\\n",
    "    llr_sequential_confmx,\\\n",
    "    truncated_MSPRT, threshold_generator,\\\n",
    "    calc_llrs, calc_oblivious_llrs, threshold_generator, thresh_sanity_check,\\\n",
    "    NP_test,\\\n",
    "    get_LLR_min_and_max, get_linspace, threshold_generator_with_values,\\\n",
    "    seqconfmx_to_metrics\n",
    "\n",
    "num_thresh = 200 # Total number of thresholds to be used to plot the SAT curve\n",
    "batch_thresh = 5 # Memory consuming!! How many thresholds to be parallely computed at once. < 10 is recommended\n",
    "assert num_thresh % batch_thresh == 0 # Make sure\n",
    "sparsity = \"linspace\" # SPRT's threshold generation method.\"linspace\" or \"logspace\".\n",
    "class_plot = 1\n",
    "dataname = 'Nosaic-MNIST'\n",
    "\n",
    "values = get_linspace(minrange, maxrange, num_thresh, sparsity)\n",
    "\n",
    "ls_confmx_th = []\n",
    "ls_mht_th = []\n",
    "ls_vht_th = []\n",
    "ls_trt_th = []\n",
    "for i in tqdm.tqdm(range(num_thresh // batch_thresh)):\n",
    "    # Generate Thresholds for SPRT\n",
    "    idx = batch_thresh * i\n",
    "    itr_values = values[idx : idx + batch_thresh]\n",
    "    itr_thresh = threshold_generator_with_values(itr_values, duration, nb_cls) # memory consuming\n",
    "    thresh_sanity_check(itr_thresh)\n",
    "    \n",
    "    # Confusion matrix of SPRT, mean/var hitting time, \n",
    "    # and truncation rate\n",
    "    ###########################\n",
    "    tmp_confmx_th, tmp_mht_th, tmp_vht_th, tmp_trt_th = \\\n",
    "        truncated_MSPRT(\n",
    "            llr_mtx=LLRpool,\n",
    "            labels_concat=labelpool,\n",
    "            thresh_mtx=itr_thresh) # Super GPU memory super-consuming if batch_thresh is large.\n",
    "        # (num thresh, num classes, num classes)\n",
    "        # (num thresh,)\n",
    "        # (num thresh,)\n",
    "        # (num thresh,)\n",
    "    ls_confmx_th.append(tmp_confmx_th)\n",
    "    ls_mht_th.append(tmp_mht_th)\n",
    "    ls_vht_th.append(tmp_vht_th)\n",
    "    ls_trt_th.append(tmp_trt_th)\n",
    "\n",
    "confmx_th = tf.concat(ls_confmx_th, axis=0)\n",
    "mht_th = tf.concat(ls_mht_th, axis=0)\n",
    "vht_th = tf.concat(ls_vht_th, axis=0)\n",
    "trt_th = tf.concat(ls_trt_th, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract recalls (sensitivities)\n",
    "dc_mtr = seqconfmx_to_metrics(confmx_th)\n",
    "\n",
    "# Output\n",
    "ls_sns = dc_mtr[\"SNS\"].numpy()\n",
    "    # (num thresh, num classes + 2)\n",
    "    # [[recall of class 0, 1, 2, ..., recall of the last class, balanced accuracy, accuracy], ...]\n",
    "\n",
    "ls_mht = mht_th.numpy()\n",
    "    # (num thresh,)\n",
    "ls_vht = vht_th.numpy()\n",
    "ls_stdht = np.sqrt(ls_vht)\n",
    "    # (num thresh,)\n",
    "ls_trt = trt_th.numpy()\n",
    "    # (num thresh,)\n",
    "\n",
    "# Parameters\n",
    "x = ls_mht\n",
    "y = 100 * (1 - ls_sns[:, class_plot]) # macro-averaged recall\n",
    "\n",
    "title = \"SAT curve, order={} (shade: standard deviation)\".format(order_sprt)\n",
    "ylabel = '100 - Recall (Mac-ave) (%)'\n",
    "xlabel = 'Mean hitting time (# frames)'\n",
    "\n",
    "\n",
    "# Size\n",
    "fig, ax = plt.subplots(figsize=(16.2, 10), facecolor='white')\n",
    "# Scale\n",
    "# ax.set_xscale('log')\n",
    "# Grid\n",
    "major_ticks = np.arange(0, duration + 1, 10)\n",
    "major_ticks[0] += 1\n",
    "minor_ticks = np.arange(0, duration + 1, 1)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_xticklabels(['0, 10, 20, 30, 40, 50'])\n",
    "for axis in [ax.xaxis, ax.yaxis]: # ?\n",
    "    axis.set_major_formatter(ScalarFormatter())\n",
    "    axis.set_minor_formatter(NullFormatter())\n",
    "\n",
    "# plot\n",
    "plt.scatter(x, y, s=20, color='orchid')\n",
    "plt.fill_betweenx(y, x-ls_stdht, x+ls_stdht, color='orchid', alpha=0.5)\n",
    "plt.ylabel(ylabel)        \n",
    "plt.xlabel(xlabel)\n",
    "plt.grid(which='both')\n",
    "plt.title(title)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('basics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "edc3f94367226733bdb5a33550172174a2790710df7d53bdbe4c7bdd3f34d8af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
