root_dir: "./"
project_name: "sprt_multiclass"
name_dataset: "nosaic_mnist"
root_tblogs: "./data-directory/sprt_multiclass/nosaic_mnist/tblogs" # where TensorBoard logs will be saved
root_dblogs: "./data-directory/sprt_multiclass/nosaic_mnist/dblogs" # where .db files generated by optuna will be saved
root_ckptlogs: "./data-directory/sprt_multiclass/nosaic_mnist/ckptlogs" # # where checkpoint files will be saved
tfr_train: "./data-directory/nosaic_mnist_train.tfrecords" 
    # tfr_train is to be split to training and validation datasets
tfr_test: "./data-directory/nosaic_mnist_test.tfrecords"
num_traindata: 50000
num_validdata: 10000
num_testdata: 10000
duration: 20
feat_dims: [28, 28, 1]


# Training config
gpu: 0
subproject_name: "FE_NMNIST"
comment: "Res1_44_32"
nb_trials: 1
exp_phase: "try"
    # try: trial run, 
    # tuning: optuning (results will be straged in .db file under `root_dglogs`), 
    # stat: same as `try` but has access to test data
    
flag_resume: False
path_resume: "./data-directory/trained_models/FE_NMNIST" 
flag_seed: False
seed: 7
train_display_step: 20
valid_step: 200
max_to_keep: 3
num_classes: 2

# Model
name_optimizer: "adam" 

resnet_size: 44 # 110 # ResNet num of layers
final_size: 32 # 128 # Num of final channels after the GAP layer
flag_wd: True 
resnet_version: 1 
    # See the ResNet's original paper.
    # V1 is for low resolution datasets (like CIFAR, MNIST).
    # V2 is for high resolution datasets (like ImageNet).

    # Reference:
    # ResNet v1
    # 8  : num residual blocks = [1, 1, 1], # num params = 0.09M, channel = (16, 32, 64)
    # 14 : [2, 2, 2],    # 0.18M
    # 20 : [3, 3, 3],    # 0.25M
    # 32 : [5, 5, 5],    # 0.46M
    # 44 : [7, 7, 7],    # 0.66M
    # 56 : [9, 9, 9],    # 0.85M
    # 110: [18, 18, 18], # 1.7M
    # 218: [36, 36, 36]  # 3.4M

# Hyperparameters
nb_epochs: 10
batch_size: 100 # =1 is not currently supported 
learning_rates: [5e-4, 1e-4, 1e-4]
decay_steps: [1500000000, 3000000000] 
weight_decay: 0.0001 

# Data properties
nb_cls: 2

# Search space for optuna 
list_lr: [1e-2, 5e-3, 1e-3, 5e-4, 1e-4] # learning rate
list_bs: [64] # batch size
list_opt: ["adam", "momentum", "rmsprop"] # optimizer
list_do: [0.] # dropout. Not supported.
list_wd: [0.001, 0.0001, 0.00001] # weight decay